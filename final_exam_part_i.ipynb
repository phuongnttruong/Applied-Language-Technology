{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "polished-module",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Final Examination (Notebook I)\n",
    "\n",
    "For instructions on how to use TestMyCode (TMC) to test your code and submit it to the server, see <a href=\"https://applied-language-technology.mooc.fi/html/tmc.html\" target=\"blank_\">here</a>.\n",
    "\n",
    "Remember to save this Notebook before testing your code. Press <kbd>Control</kbd>+<kbd>s</kbd> or select the *File* menu and click *Save*.\n",
    "\n",
    "**The maximum number of points for this Notebook is 45.**\n",
    "\n",
    "⚠️ Set the variable `grade` in the cell below to `True` to enable testing. ⚠️\n",
    "\n",
    "The commands `tmc test` and `tmc submit` work only when the `grade` variable has been set to `True`. \n",
    "\n",
    "You can disable testing for some Notebooks to speed up the process before submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3561f369",
   "metadata": {
    "deletable": false,
    "test": "grade"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThis Notebook will be graded.\u001b[0m"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the value of the variable 'grade' to True to enable testing and submitting\n",
    "grade = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67959323",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## 1. Prepare text for processing using Stanza (15 points)\n",
    "\n",
    "**Prerequisites for this exercise**: None.\n",
    "\n",
    "The directory `data` contains 10 articles from the Estonian Wikipedia, whose filenames follow the pattern et_wiki_X.txt, in which X stands for a number that identifies the article.\n",
    "\n",
    "Open each file, read the contents and store the resulting string objects into a list named texts.\n",
    "\n",
    "Prepare these texts for processing using Stanza by creating Document objects without annotations.\n",
    "\n",
    "Store the resulting Document objects into a list named `docs_in`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e7d5aff0",
   "metadata": {
    "deletable": false,
    "test": "prepare_text"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"docs_in\" was defined successfully! 1 point.\u001b[0m"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe list under the variable \"docs_in\" contains the expected number of items! 1 point.\u001b[0m"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"docs_in\" contains Stanza Document objects! 13 points.\u001b[0m"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your answer below this line. Please enter your entire solution in this cell.\n",
    "#import stanza\n",
    "\n",
    "# Initialize Stanza with the desired language model (Estonian in this case)\n",
    "\n",
    "nlp = stanza.Pipeline(processors='tokenize, pos')  # Create the pipeline\n",
    "\n",
    "# List to store the resulting Document objects\n",
    "docs_in = []\n",
    "\n",
    "# Loop through the files and process each one\n",
    "for i in range(1, 11):  # Assuming there are 10 files with numbers from 1 to 10\n",
    "    filename = f'data/et_wiki_{i}.txt'  # Adjust the path as needed\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()  # Read the contents of the file\n",
    "    \n",
    "    # Create a Stanza Document object without annotations\n",
    "    doc = nlp(text)\n",
    "    docs_in.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3cf6e0-b76c-4d2f-9672-7b4f6bbd4c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "successful-phase",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## 2. Get sentences from Stanza *Document* objects (15 points)\n",
    "\n",
    "**Prerequisites for this exercise**: None.\n",
    "\n",
    "The directory `data` contains a file named `docs.pkl`, which contains 10 articles from the Estonian Wikipedia that have been processed using Stanza. The code provided in the cell below loads these Document objects and stores them into a list named `docs`.\n",
    "\n",
    "Assume that we want to study the linguistic features of introductory sentences in Estonian Wikipedia. \n",
    "\n",
    "The first sentence in each *Document* object in the list `docs` corresponds to the title of the article, which means we must retrieve the second sentence in the *Document*.\n",
    "\n",
    "Collect the second sentence of each *Document* object into a list named `intros`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "terminal-shark",
   "metadata": {
    "deletable": false,
    "test": "get_sents"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"intros\" was defined successfully! 1 point.\u001b[0m"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"intros\" contains Stanza Sentence objects! 1 point.\u001b[0m"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"intros\" contains the expected values! 13 points.\u001b[0m"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the 'pickle' module from Python for serializing data\n",
    "import pickle\n",
    "\n",
    "# Open the file with pickled Stanza Document objects for reading\n",
    "with open('data/docs.bin', mode='rb') as f:\n",
    "    \n",
    "    # Load the pickled Documents and assign under variable 'docs'\n",
    "    docs = pickle.load(f)\n",
    "\n",
    "# Write your answer below this line. Please enter your entire solution in this cell.\n",
    "intros = []\n",
    "\n",
    "# Iterate over each Document object in the 'docs' list\n",
    "for doc in docs:\n",
    "    # Check if the document has at least two sentences\n",
    "    if len(doc.sentences) >= 2:\n",
    "        # Get the second sentence (index 1, since Python is 0-based)\n",
    "        second_sentence = doc.sentences[1]\n",
    "        intros.append(second_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb89aa-5397-46a5-8c0b-83d5ba150d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beginning-greensboro",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## 3. Retrieve copulas from *Sentence* objects (15 points)\n",
    "\n",
    "**Prerequisites for this exercise**: None. You can use the variable `docs` defined in exercise 2.\n",
    "\n",
    "Loop over the *Sentence* objects in the *Document* objects, which are stored in the list `docs`.\n",
    "\n",
    "Create a list named `copulas` to hold the information collected below.\n",
    "\n",
    "Convert the *Sentence* objects into Python dictionaries to access the linguistic annotations. \n",
    "\n",
    "Examine each token in every *Sentence* object. If the token has the dependency relation `cop` (short for [copula](https://universaldependencies.org/u/dep/cop.html)), add the **form** of the token (under `text`) to the list `copulas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "editorial-birthday",
   "metadata": {
    "deletable": false,
    "test": "get_cops"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"copulas\" was defined successfully! 1 point.\u001b[0m"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe list under the variable \"copulas\" contains the expected number of items! 1 point.\u001b[0m"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe list \"copulas\" contains the expected items! 13 points.\u001b[0m"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your answer below this line. Please enter your entire solution in this cell.\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model (you may need to download it first)\n",
    "\n",
    "\n",
    "# Assuming 'docs' is a list of Document objects with text as a single string\n",
    "copulas = []\n",
    "doc_1 =[]\n",
    "for sentence in docs:\n",
    "    doc_1.append(sentence.to_dict())\n",
    "for i in doc_1:\n",
    "    for k in i:\n",
    "        for j in k:\n",
    "            if j['deprel'] == 'cop':\n",
    "                copulas.append(j['text'])\n",
    "                \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5eac4afb-53f1-4d96-8786-7381c41f77e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oli',\n",
       " 'oli',\n",
       " 'on',\n",
       " 'on',\n",
       " 'on',\n",
       " 'on',\n",
       " 'on',\n",
       " 'olid',\n",
       " 'oli',\n",
       " 'oli',\n",
       " 'polnud',\n",
       " 'oli',\n",
       " 'on',\n",
       " 'oli',\n",
       " 'oli',\n",
       " 'olid',\n",
       " 'oli',\n",
       " 'oli',\n",
       " 'oli',\n",
       " 'olnud',\n",
       " 'on',\n",
       " 'oli',\n",
       " 'oli',\n",
       " 'on',\n",
       " 'on',\n",
       " 'on',\n",
       " 'on',\n",
       " 'on',\n",
       " 'on',\n",
       " 'on',\n",
       " 'oli',\n",
       " 'olla',\n",
       " 'oli',\n",
       " 'oli',\n",
       " 'olid',\n",
       " 'oli',\n",
       " 'on',\n",
       " 'on',\n",
       " 'oli',\n",
       " 'on',\n",
       " 'on',\n",
       " 'on',\n",
       " 'oli',\n",
       " 'oli',\n",
       " 'oli',\n",
       " 'on',\n",
       " 'on',\n",
       " 'on',\n",
       " 'on',\n",
       " 'on',\n",
       " 'on',\n",
       " 'on']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a0992f-af7f-4e9e-b33f-8677589fb71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea1103-4e49-4f98-b497-bb27bc3f440e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
