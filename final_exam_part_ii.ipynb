{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "billion-paraguay",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Final Examination (Notebook II)\n",
    "\n",
    "For instructions on how to use TestMyCode (TMC) to test your code and submit it to the server, see <a href=\"https://applied-language-technology.mooc.fi/html/tmc.html\" target=\"blank_\">here</a>.\n",
    "\n",
    "Remember to save this Notebook before testing your code. Press <kbd>Control</kbd>+<kbd>s</kbd> or select the *File* menu and click *Save*.\n",
    "\n",
    "**The maximum number of points for this Notebook is 45.**\n",
    "\n",
    "⚠️ Set the variable `grade` in the cell below to `True` to enable testing. ⚠️\n",
    "\n",
    "The commands `tmc test` and `tmc submit` work only when the `grade` variable has been set to `True`. \n",
    "\n",
    "You can disable testing for some Notebooks to speed up the process before submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ddf840e",
   "metadata": {
    "deletable": false,
    "test": "grade"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThis Notebook will be graded.\u001b[0m"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the value of the variable 'grade' to True to enable testing and submitting\n",
    "grade = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-consortium",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## 1. Load a text file and process it using spaCy (15 points)\n",
    "\n",
    "**Prerequisites for this exercise**: None.\n",
    "\n",
    "Import the spaCy library and load a small language model for English. Assign the resulting *Language* object under the variable `nlp_en`.\n",
    "\n",
    "The directory `data` contains a file named `en_wiki.txt`. Open this file for reading, read the contents and store the resulting string object under the variable `text`. \n",
    "\n",
    "Then feed the string object under `text` to the spaCy *Language* object `nlp_en`. Store the resulting *Doc* object under the variable `wikidoc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "distributed-picture",
   "metadata": {
    "deletable": false,
    "test": "process_text"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"nlp_en\" was defined successfully! 1 point.\u001b[0m"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"wikidoc\" was defined successfully! 1 point.\u001b[0m"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"wikidoc\" contains the expected values! 13 points.\u001b[0m"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your answer below this line. Please enter your entire solution in this cell.\n",
    "import spacy\n",
    "\n",
    "# Load a small language model for English\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Specify the path to the text file\n",
    "file_path = \"data/en_wiki.txt\"\n",
    "\n",
    "# Read the contents of the text file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "    \n",
    "\n",
    "# Process the text using spaCy\n",
    "wikidoc = nlp_en(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-camping",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## 2. Match patterns based on part-of-speech tags (15 points)\n",
    "\n",
    "**Prerequisites for this exercise**: You must have completed exercise 1 in this Notebook. You can use variables defined in exercise 1.\n",
    "\n",
    "Define a pattern rule for matching sequences of *Tokens* that have a **noun** as their coarse part-of-speech tag and **compound** as their syntactic dependency. In addition to individual *Tokens*, instruct spaCy to return matching sequences of *Tokens* that occur one or more times in the text.\n",
    "\n",
    "Import the *Matcher* class and initialise a *Matcher* object using the *Vocabulary* of the *Language* object under `nlp_en`. Store the *Matcher* under the variable `n_matcher`.\n",
    "\n",
    "Add the pattern rule to the *Matcher* `n_matcher`. Instruct spaCy to return the longest sequence of matches only as spaCy *Span* objects.\n",
    "\n",
    "Apply the *Matcher* to the *Doc* object under the variable `wikidoc` and store the resulting matches under the variable `n_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "strange-blair",
   "metadata": {
    "deletable": false,
    "test": "match_pos"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"n_matcher\" was defined successfully! 1 point.\u001b[0m"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"n_results\" was defined successfully! 1 point.\u001b[0m"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"n_results\" contains the expected values! 13 points.\u001b[0m"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your answer below this line. Please enter your entire solution in this cell.\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "\n",
    "# Initialize a DependencyMatcher object using the vocabulary of the Language object nlp_en\n",
    "n_matcher = Matcher(nlp_en.vocab)\n",
    "pattern = [\n",
    "    {\"POS\": \"NOUN\", \"DEP\": \"compound\", \"OP\": \"+\"}\n",
    "]\n",
    "\n",
    "n_matcher.add('noun_compound', [pattern], greedy='LONGEST')\n",
    "\n",
    "n_results = n_matcher(wikidoc, as_spans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cafb865a-6401-4d79-98ee-d9d71143e6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Van der Waals,\n",
       " temperature separation,\n",
       " integer transition,\n",
       " oil drilling,\n",
       " class blimp,\n",
       " temperature gas,\n",
       " hydrogen rocket,\n",
       " peak wartime,\n",
       " helium electron,\n",
       " electron cloud,\n",
       " der Waals,\n",
       " carbon cage,\n",
       " diamond anvil,\n",
       " proton chain,\n",
       " helium tube,\n",
       " world helium,\n",
       " gas well,\n",
       " air distillation,\n",
       " world helium,\n",
       " heat capacity,\n",
       " ground support,\n",
       " heat capacity,\n",
       " girl singing,\n",
       " emergency press,\n",
       " Helium,\n",
       " chemical,\n",
       " gas,\n",
       " boiling,\n",
       " line,\n",
       " helium,\n",
       " uranium,\n",
       " gas,\n",
       " silicon,\n",
       " helium,\n",
       " quantum,\n",
       " helium,\n",
       " alpha,\n",
       " helium-4,\n",
       " gas,\n",
       " astronomer,\n",
       " line,\n",
       " chemist,\n",
       " physicist,\n",
       " mineral,\n",
       " earth,\n",
       " mineral,\n",
       " physicist,\n",
       " mineral,\n",
       " alpha,\n",
       " glass,\n",
       " physicist,\n",
       " ζ,\n",
       " hydrogen,\n",
       " helium,\n",
       " integer,\n",
       " quantum,\n",
       " physicist,\n",
       " physicists,\n",
       " helium-3,\n",
       " gas,\n",
       " state,\n",
       " %,\n",
       " %,\n",
       " %,\n",
       " gas,\n",
       " helium,\n",
       " barrage,\n",
       " %,\n",
       " C,\n",
       " extraction,\n",
       " arc,\n",
       " mass,\n",
       " production,\n",
       " lift,\n",
       " helium,\n",
       " conservation,\n",
       " gas,\n",
       " helium,\n",
       " nitrogen,\n",
       " gas,\n",
       " %,\n",
       " %,\n",
       " %,\n",
       " extraction,\n",
       " helium,\n",
       " helium,\n",
       " gas,\n",
       " helium,\n",
       " helium,\n",
       " helium,\n",
       " helium,\n",
       " helium,\n",
       " supply,\n",
       " quantum,\n",
       " hydrogen,\n",
       " body,\n",
       " chemistry,\n",
       " charge,\n",
       " helium-4,\n",
       " alpha,\n",
       " energy,\n",
       " charge,\n",
       " electron,\n",
       " helium,\n",
       " chemical,\n",
       " helium,\n",
       " boiling,\n",
       " helium-4,\n",
       " particle,\n",
       " fusion,\n",
       " helium-4,\n",
       " compound,\n",
       " helium-4,\n",
       " helium-4,\n",
       " beta,\n",
       " alpha,\n",
       " parts,\n",
       " part,\n",
       " gas,\n",
       " fraction,\n",
       " inversion,\n",
       " expansion,\n",
       " plasma,\n",
       " quantum,\n",
       " point,\n",
       " −272,\n",
       " melting,\n",
       " room,\n",
       " boiling,\n",
       " lambda,\n",
       " isotope,\n",
       " liquid,\n",
       " helium,\n",
       " lambda,\n",
       " helium,\n",
       " boiling,\n",
       " lambda,\n",
       " fluid,\n",
       " lambda,\n",
       " helium,\n",
       " ground,\n",
       " helium,\n",
       " fountain,\n",
       " helium,\n",
       " helium,\n",
       " helium,\n",
       " equilibrium,\n",
       " helium,\n",
       " helium,\n",
       " helium,\n",
       " heat,\n",
       " quantum,\n",
       " valence,\n",
       " valence,\n",
       " wave,\n",
       " sound,\n",
       " helium,\n",
       " surface,\n",
       " gravity,\n",
       " formation,\n",
       " alpha,\n",
       " alpha,\n",
       " helium-4,\n",
       " trace,\n",
       " Trace,\n",
       " beta,\n",
       " isotope,\n",
       " Extraplanetary,\n",
       " trace,\n",
       " K,\n",
       " boiling,\n",
       " helium-3,\n",
       " quantum,\n",
       " helium-4,\n",
       " helium-3,\n",
       " Dilution,\n",
       " helium,\n",
       " helium,\n",
       " gamma,\n",
       " energy,\n",
       " ionization,\n",
       " ions,\n",
       " ground,\n",
       " molecule,\n",
       " band,\n",
       " polarization,\n",
       " helium,\n",
       " helium,\n",
       " helium,\n",
       " oxygen,\n",
       " FHeO−,\n",
       " fullerene,\n",
       " resonance,\n",
       " helium,\n",
       " chemical,\n",
       " pressures,\n",
       " nitrogen,\n",
       " room,\n",
       " pressures,\n",
       " baryonic,\n",
       " proton,\n",
       " alpha,\n",
       " helium,\n",
       " mineral,\n",
       " gas,\n",
       " helium,\n",
       " scale,\n",
       " %,\n",
       " boiling,\n",
       " helium,\n",
       " purification,\n",
       " A,\n",
       " production,\n",
       " helium,\n",
       " distance,\n",
       " helium,\n",
       " helium,\n",
       " helium,\n",
       " gas,\n",
       " helium,\n",
       " pie,\n",
       " reserve,\n",
       " neon,\n",
       " helium,\n",
       " helium,\n",
       " velocity,\n",
       " ISO,\n",
       " pressure,\n",
       " pressure,\n",
       " tube,\n",
       " helium,\n",
       " laureate,\n",
       " market,\n",
       " helium,\n",
       " helium,\n",
       " helium,\n",
       " boiling,\n",
       " MRI,\n",
       " NMR,\n",
       " leak,\n",
       " germanium,\n",
       " gas,\n",
       " wind,\n",
       " shielding,\n",
       " shielding,\n",
       " heat,\n",
       " leak,\n",
       " tracer,\n",
       " vacuum,\n",
       " pressure,\n",
       " helium,\n",
       " leak,\n",
       " measurement,\n",
       " Helium,\n",
       " gas,\n",
       " permeation,\n",
       " permeation,\n",
       " hydrogen,\n",
       " ullage,\n",
       " displace,\n",
       " storage,\n",
       " rocket,\n",
       " purge,\n",
       " space,\n",
       " breathing,\n",
       " breathing,\n",
       " oxygen,\n",
       " psychomotor,\n",
       " oxygen,\n",
       " neon,\n",
       " gas,\n",
       " barcode,\n",
       " laser,\n",
       " neutron,\n",
       " reactor,\n",
       " heat,\n",
       " transfer,\n",
       " refrigeration,\n",
       " ozone,\n",
       " disk,\n",
       " temperature,\n",
       " telescope,\n",
       " carrier,\n",
       " gas,\n",
       " cryogenics,\n",
       " resonance,\n",
       " helium,\n",
       " trace,\n",
       " resonant,\n",
       " party,\n",
       " flow,\n",
       " lung,\n",
       " helium,\n",
       " helium,\n",
       " helium,\n",
       " TV,\n",
       " air,\n",
       " air,\n",
       " safety,\n",
       " hotel,\n",
       " safety,\n",
       " expansion,\n",
       " pressure,\n",
       " relief,\n",
       " helium,\n",
       " anesthetic]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-piece",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## 3. Match patterns based on syntactic dependencies (15 points)\n",
    "\n",
    "**Prerequisites for this exercise**: You must have completed exercise 1 in this Notebook. You can use variables defined in exercise 1.\n",
    "\n",
    "Define a pattern rule for matching Tokens that have a **verb** as their coarse part-of-speech tag, and their **subjects** and **objects**.\n",
    "\n",
    "Import the *DependencyMatcher* class and initialise a *DependencyMatcher* object using the *Vocabulary* of the *Language* object under `nlp_en`. Store the *DependencyMatcher* under the variable `d_matcher`.\n",
    "\n",
    "Add the pattern rule to the *DependencyMatcher* `d_matcher`.\n",
    "\n",
    "Apply the *DependencyMatcher* to the *Doc* object `wikidoc` and store the resulting matches under the variable `d_results`.\n",
    "\n",
    "*Tip*: Use the verb as the anchor pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "skilled-noise",
   "metadata": {
    "deletable": false,
    "test": "match_dep"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"d_matcher\" was defined successfully! 1 point.\u001b[0m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"d_results\" was defined successfully! 1 point.\u001b[0m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[TMC]\u001b[0m \u001b[92mThe variable \"d_results\" contains the expected values! 13 points.\u001b[0m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your answer below this line. Please enter your entire solution in this cell.\n",
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "# Initialize a DependencyMatcher object using the vocabulary of the Language object nlp_en\n",
    "d_matcher = DependencyMatcher(nlp_en.vocab)\n",
    "pattern = [\n",
    "    {\n",
    "        'RIGHT_ID': 'verb',\n",
    "        'RIGHT_ATTRS': {'POS': 'VERB'}\n",
    "    },\n",
    "    {\n",
    "        'LEFT_ID': 'verb',  \n",
    "        'REL_OP': '>',     \n",
    "        'RIGHT_ID': 'subject',\n",
    "        'RIGHT_ATTRS': {'DEP': {'in': ['nsubj']}} \n",
    "    },\n",
    "    {\n",
    "        'LEFT_ID': 'verb',  \n",
    "        'REL_OP': '>',      \n",
    "        'RIGHT_ID': 'object',\n",
    "        'RIGHT_ATTRS': {'DEP': {'in': ['dobj']}}  \n",
    "    }\n",
    "]\n",
    "\n",
    "d_matcher.add('verb_subject_object', [pattern])\n",
    "\n",
    "d_results = d_matcher(wikidoc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e803f6b-29ae-4d01-ad49-589343b17ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18120831537121313479, [302, 301, 306]),\n",
       " (18120831537121313479, [316, 315, 317]),\n",
       " (18120831537121313479, [339, 338, 337]),\n",
       " (18120831537121313479, [365, 364, 367]),\n",
       " (18120831537121313479, [826, 821, 829]),\n",
       " (18120831537121313479, [838, 837, 835]),\n",
       " (18120831537121313479, [838, 837, 840]),\n",
       " (18120831537121313479, [878, 877, 880]),\n",
       " (18120831537121313479, [902, 901, 903]),\n",
       " (18120831537121313479, [918, 917, 920]),\n",
       " (18120831537121313479, [996, 995, 1000]),\n",
       " (18120831537121313479, [1002, 1001, 1005]),\n",
       " (18120831537121313479, [1058, 1057, 1059]),\n",
       " (18120831537121313479, [1091, 1090, 1094]),\n",
       " (18120831537121313479, [1108, 1104, 1110]),\n",
       " (18120831537121313479, [1120, 1115, 1123]),\n",
       " (18120831537121313479, [1156, 1154, 1161]),\n",
       " (18120831537121313479, [1260, 1259, 1263]),\n",
       " (18120831537121313479, [1269, 1268, 1271]),\n",
       " (18120831537121313479, [1302, 1299, 1303]),\n",
       " (18120831537121313479, [1334, 1333, 1336]),\n",
       " (18120831537121313479, [1446, 1444, 1452]),\n",
       " (18120831537121313479, [1472, 1466, 1474]),\n",
       " (18120831537121313479, [1514, 1513, 1517]),\n",
       " (18120831537121313479, [1616, 1615, 1619]),\n",
       " (18120831537121313479, [1630, 1627, 1631]),\n",
       " (18120831537121313479, [1661, 1660, 1677]),\n",
       " (18120831537121313479, [1750, 1749, 1753]),\n",
       " (18120831537121313479, [1776, 1775, 1781]),\n",
       " (18120831537121313479, [1867, 1866, 1870]),\n",
       " (18120831537121313479, [2003, 1998, 2008]),\n",
       " (18120831537121313479, [2042, 2041, 2044]),\n",
       " (18120831537121313479, [2053, 2051, 2056]),\n",
       " (18120831537121313479, [2176, 2174, 2177]),\n",
       " (18120831537121313479, [2190, 2189, 2199]),\n",
       " (18120831537121313479, [2299, 2296, 2303]),\n",
       " (18120831537121313479, [2378, 2377, 2382]),\n",
       " (18120831537121313479, [2404, 2393, 2406]),\n",
       " (18120831537121313479, [2431, 2414, 2432]),\n",
       " (18120831537121313479, [2635, 2634, 2642]),\n",
       " (18120831537121313479, [2651, 2649, 2653]),\n",
       " (18120831537121313479, [2693, 2692, 2694]),\n",
       " (18120831537121313479, [2856, 2852, 2858]),\n",
       " (18120831537121313479, [2873, 2872, 2870]),\n",
       " (18120831537121313479, [2914, 2913, 2916]),\n",
       " (18120831537121313479, [2943, 2942, 2946]),\n",
       " (18120831537121313479, [2961, 2949, 2966]),\n",
       " (18120831537121313479, [2995, 2993, 2997]),\n",
       " (18120831537121313479, [3011, 3010, 3016]),\n",
       " (18120831537121313479, [3153, 3152, 3158]),\n",
       " (18120831537121313479, [3208, 3207, 3210]),\n",
       " (18120831537121313479, [3271, 3270, 3273]),\n",
       " (18120831537121313479, [3383, 3382, 3386]),\n",
       " (18120831537121313479, [3442, 3441, 3445]),\n",
       " (18120831537121313479, [3554, 3549, 3558]),\n",
       " (18120831537121313479, [3658, 3657, 3666]),\n",
       " (18120831537121313479, [3705, 3688, 3709]),\n",
       " (18120831537121313479, [3746, 3745, 3749]),\n",
       " (18120831537121313479, [3946, 3945, 3948]),\n",
       " (18120831537121313479, [3998, 3997, 4002]),\n",
       " (18120831537121313479, [4049, 4048, 4051]),\n",
       " (18120831537121313479, [4099, 4098, 4102]),\n",
       " (18120831537121313479, [4180, 4179, 4185]),\n",
       " (18120831537121313479, [4215, 4214, 4219]),\n",
       " (18120831537121313479, [4279, 4278, 4281]),\n",
       " (18120831537121313479, [4329, 4319, 4332]),\n",
       " (18120831537121313479, [4359, 4357, 4362]),\n",
       " (18120831537121313479, [4425, 4424, 4428]),\n",
       " (18120831537121313479, [4454, 4453, 4455]),\n",
       " (18120831537121313479, [4652, 4651, 4653]),\n",
       " (18120831537121313479, [4655, 4650, 4658]),\n",
       " (18120831537121313479, [4671, 4670, 4675]),\n",
       " (18120831537121313479, [4736, 4734, 4739]),\n",
       " (18120831537121313479, [4784, 4783, 4787]),\n",
       " (18120831537121313479, [4821, 4819, 4823]),\n",
       " (18120831537121313479, [4877, 4876, 4879]),\n",
       " (18120831537121313479, [5088, 5082, 5090]),\n",
       " (18120831537121313479, [5167, 5158, 5169]),\n",
       " (18120831537121313479, [5183, 5182, 5184]),\n",
       " (18120831537121313479, [5248, 5247, 5250]),\n",
       " (18120831537121313479, [5264, 5263, 5268]),\n",
       " (18120831537121313479, [5271, 5259, 5274]),\n",
       " (18120831537121313479, [5301, 5300, 5304]),\n",
       " (18120831537121313479, [5319, 5318, 5321]),\n",
       " (18120831537121313479, [5383, 5381, 5386]),\n",
       " (18120831537121313479, [5417, 5416, 5419]),\n",
       " (18120831537121313479, [5447, 5446, 5450]),\n",
       " (18120831537121313479, [5471, 5469, 5473]),\n",
       " (18120831537121313479, [5513, 5512, 5514]),\n",
       " (18120831537121313479, [5574, 5572, 5576]),\n",
       " (18120831537121313479, [5586, 5583, 5590]),\n",
       " (18120831537121313479, [5593, 5592, 5596]),\n",
       " (18120831537121313479, [5818, 5817, 5820]),\n",
       " (18120831537121313479, [5889, 5888, 5892]),\n",
       " (18120831537121313479, [6096, 6095, 6098]),\n",
       " (18120831537121313479, [6195, 6193, 6197]),\n",
       " (18120831537121313479, [6195, 6193, 6202]),\n",
       " (18120831537121313479, [6335, 6333, 6341]),\n",
       " (18120831537121313479, [6345, 6344, 6349]),\n",
       " (18120831537121313479, [6475, 6473, 6477]),\n",
       " (18120831537121313479, [6489, 6488, 6495]),\n",
       " (18120831537121313479, [6574, 6557, 6577]),\n",
       " (18120831537121313479, [6729, 6727, 6731]),\n",
       " (18120831537121313479, [6958, 6957, 6963]),\n",
       " (18120831537121313479, [6973, 6972, 6975]),\n",
       " (18120831537121313479, [7032, 7031, 7033]),\n",
       " (18120831537121313479, [7208, 7207, 7209]),\n",
       " (18120831537121313479, [7296, 7294, 7299]),\n",
       " (18120831537121313479, [7443, 7442, 7446]),\n",
       " (18120831537121313479, [7613, 7611, 7615]),\n",
       " (18120831537121313479, [7681, 7680, 7683]),\n",
       " (18120831537121313479, [7769, 7763, 7772]),\n",
       " (18120831537121313479, [7863, 7862, 7866]),\n",
       " (18120831537121313479, [7885, 7884, 7887]),\n",
       " (18120831537121313479, [7901, 7900, 7902]),\n",
       " (18120831537121313479, [7970, 7955, 7973]),\n",
       " (18120831537121313479, [7975, 7974, 7977]),\n",
       " (18120831537121313479, [8009, 8006, 8011]),\n",
       " (18120831537121313479, [8066, 8063, 8068]),\n",
       " (18120831537121313479, [8097, 8094, 8100]),\n",
       " (18120831537121313479, [8160, 8159, 8161]),\n",
       " (18120831537121313479, [8234, 8231, 8237]),\n",
       " (18120831537121313479, [8257, 8255, 8259]),\n",
       " (18120831537121313479, [8385, 8384, 8387]),\n",
       " (18120831537121313479, [8624, 8607, 8626]),\n",
       " (18120831537121313479, [8810, 8806, 8814]),\n",
       " (18120831537121313479, [8853, 8851, 8855]),\n",
       " (18120831537121313479, [8898, 8876, 8899]),\n",
       " (18120831537121313479, [8973, 8971, 8974]),\n",
       " (18120831537121313479, [8999, 8998, 9001])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3adf0f-266f-4c90-8114-31829e30579c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
